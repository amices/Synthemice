---
title: "Outline paper - Multiple imputation for statistical disclosure control: Creating synthetic datasets with *mice*"
subtitle: "Anony*mice*d shareable data: Using *mice* to create multiply imputed synthetic datasets" 
author:
- Thom Volker & Gerko Vink
date: "`r format(Sys.time(), '%d-%m-%Y')`"
output: 
  html_document:
    theme: spacelab
    highlight: tango
    toc: true
    toc_depth: 2
    toc_float: true
    number_sections: true
bibliography: federated_imp.bib
csl: "apa-6th-edition.csl"
---

# OUTLINE

## Introduction

- [x] What is synthetic data?
- [x] Why is it useful?
- [x] Why should it be easy to generate?
- [x] Parallel to missing data
- [x] If similar to missing data, then why not use `mice`?
- [x] `mice` can only be exploited if the statistical properties are correct.
- [x] In this paper we demonstrate a straightforward workflow for generating synthetic datasets and we investigate how to obtain valid inferences from synthesized data.

## Generating synthetic data with `mice`

- [x] Explain FCS in short.
- [x] Demonstrate procedure with `where-matrix` and explain rationale.
- [ ] Demonstrate that this is a reasonable approach, based on a figure that shows the observed and synthetic relationships side-by-side (e.g. `wgt ~ age + gen`).
- [ ] Observed values can be regarded as starting values for the imputation model. Since the observed data is used for creating the imputation models, no iterations are required. That is, the model is fixed, and not dependent on the imputed values.

## Drawing inferences from `mice` synthesized data

- [ ] What ways are there (with minimal - if any - escapes to partially/fully synthetic data).
- [ ] If based on your earlier work, cite the online reference for your simulations and give URL. No need to go into detail about it here.

### Simulation

- [ ] Simulation set-up.
- [ ] Evaluation criteria explained: the bias, coverage and CIW triangle.
- [ ] Table which demonstrates bias, coverage and CIW for all methods.

## Discussion

- [ ] Summarize findings.
- [ ] Stress that only holds for sampled data. Explain that for fully observed populations, one would need no sampling variance.
- [ ] Revisit the simplicity and the readily available solution within the `mice` workflow (if adjusted pool is required, add to mice and direct to that function).


# Introduction

<!-- 
WHY WE NEED SYNTHETIC DATA - PARAGRAPHS FROM Lazer, D., Pentland, A. S., Adamic, L., Aral, S., Barabasi, A. L., Brewer, D., Gutmann, M. (2009). Life in the network: The coming age of computational social science. Science, 323(5915), 721. http://doi.org/10.1111/j.1083-6101.2007.00367.x 


There are also enormous institutional obsta-cles to advancing a computational social sci-ence.  In  terms  of  approach,  the  subjects  ofinquiry in physics and biology present differentchallenges  to  observation  and  intervention.Quarks and cells neither mind when we dis-cover their secrets nor protest if we alter theirenvironments during the discovery process. Asfor infrastructure, the leap from social scienceto a computational social science is larger thanfrom  biology  to  a  computational  biology,largely due to the requirements of distributedmonitoring, permission seeking, and encryp-tion. There are fewer resources available in thesocial  sciences,  and  even  the  physical  (andadministrative) distance between social sciencedepartments and engineering or computer sci-ence departments tends to be greater than forthe other sciences. Perhaps the thorniest challenges exist on thedata side, with respect to access and privacy.Much of these data are proprietary (e.g., mobilephone and financial transactional information).The debacle following AOL’s public release of“anonymized” search records of many of itscustomers highlights the potential risk to indi-viduals and corporations in the sharing of per-sonal data by private companies (14). Robustmodels  of  collaboration  and  data  sharingbetween industry and academia are needed tofacilitate research and safeguard consumer pri-vacy and provide liability protection for corpo-rations. More generally, properly managing pri-vacy  issues  is  essential.  As  the  recent  U.S.National  Research  Council’s  report  on  geo-graphical information system data highlights, itis often possible to pull individual profiles outof even carefully anonymized data (15). Lastyear, the U.S. National Institutes of Health andthe Wellcome Trust abruptly removed a num-ber of genetic databases from online access(16). These databases were seemingly anony-mized,  simply  reporting  the  aggregate  fre-quency of particular genetic markers. How-ever, research revealed the potential for de-anonymization, based on the statistical powerof the sheer quantity of data collected fromeach individual in the database (17).Because a single dramatic incident involv-ing a breach of privacy could produce rulesand statutes that stifle the nascent field of com-putational  social  science,  a  self-regulatoryregime of procedures, technologies, and rulesis needed that reduces this risk but preservesresearch potential. As a cornerstone of such aself-regulatory regime, U.S. Institutional Re-view Boards (IRBs) must increase their techni-cal knowledge to understand the potential forintrusion and individual harm because newpossibilities do not fit their current paradigmsfor harm. Many IRBs would be poorly equip-ped to evaluate the possibility that complexdata could be de-anonymized. Further, it maybe necessary for IRBs to oversee the creationof  a  secure,  centralized  data  infrastructure.Currently,  existing  data  sets  are  scatteredamong many groups, with uneven skills andunderstanding  of  data  security  and  widelyvarying  protocols.  Researchers  themselvesmust develop technologies that protect privacywhile preserving data essential for research.These systems, in turn, may prove useful forindustry in managing customer privacy anddata security (18). -->


Open science, including open data, has been marked as the future of science [@gewin_data_2016], and the advantages of publicly available research data are numerous [@molloy_open_2011; @walport_brest_sharing_2011]. Collecting research data requires an enormous investment both in terms of time and monetary resources. Openly accessible research data bears the potential of increasing the scientific returns for the same data collection effort. Additionally, the fact that public funds are used for data collection results in increasing demand for the collected data. Nevertheless, the possibilities to distribute research data directly are often very limited due to restrictions on data privacy and data confidentiality. Although these regulations are much needed, privacy constraints are also ranked among the toughest challenges to overcome in the advancement of modern day social science research [@lazer_life_2009].

Anonymizing research data might seem a quick and appealing approach to limit the unique identification of participants. However, this approach is not sufficient to fulfil contemporary privacy and confidentiality requirements [@ohm_broken_2009; @national_putting_2007]. Over the years, several other techniques have been used to increase the confidentiality of research data, such as categorizing continuous variables, top coding values above an upper bound or adding random noise to the observed values [@drechsler_synthetic_2011]. However, these methods may distort the true data relation between variables, thereby reducing the data quality and the scientific returns for re-using the same data for further research. 

An alternative solution has been proposed separately by @rubin_statistical_disclosure_1993 and @little_statistical_1993. Although their approaches differ to some extent, the overarching procedure is to use bonafide observed data to generate multiply imputed synthetic datasets that can be freely disclosed. While in practice, one could see this as replacing the observed data values by multiple draws from the posterior predictive distribution of the observed data, based on some imputation model, Rubin would argue that these synthetic data values are merely draws from the same true data generating model. In that sense, the observed data is never replaced, but the population is resampled from the information captured in the (incomplete) sample. Using this approach, the researcher could replace the observed dataset as a whole with multiple synthetic versions. Alternatively, the researcher could opt to only replace a subset of the observed data. For example, one can choose to only replace dimensions in the data that could be compared with publicly available datasets or registers. Likewise, synthetisation could be limited to those values that are disclosive, such as high incomes or high turnovers. <!-- I don't think so, because the synthetic values can only be linked to observed values from other sources if they are exactly equal, or if they are the only value within a certain range. The latter might be possible when the number of values is very small, or when the observed value is so extreme that synthetic versions will still be so extreme. However, if the other values are synthetic as well, it is quite likely that synthetic information does not apply to the real-world observation. I think that the highest risk is in the possibility to extract a range of possible values, which might be uniquely match some other record that is available. -->

Conceptually, the synthetic data framework is based upon the building blocks of multiple imputation of missing data, as proposed by @rubin_multiple_1987. Instead of replacing just the missing values with multiple draws from the posterior predictive distribution, one could easily *overimpute* any observed sensitive values. Similarly to multiple imputation of missing data, the multiple synthetic datasets allow for correct statistical inferences, **regardless of the fact that the analyses do not use the "true" value**. The analyses over multiple synthetic datasets should be pooled into a single inference, so that the researcher can draw valid conclusions from the pooled results. To that respect, the variance should reflect the added variability that is induced by the imputation procedure. 

Potentially, this approach could fulfill the needs for openly accessibly data, without running into barriers with regard to privacy and confidentiality constraints. However, there is no such thing as a free lunch: data collectors have to put effort in creating high-quality synthetic data. Also, the quality of the synthetic data is highly dependent on the imputation models, and using flawed models to generate synthetic data might bias subsequent analyses. Conversely, if the models used to create the synthetic data are able to preserve the relationships between the variables as in the original data, the synthetic data can be nearly as informative as the observed data. Thus, to fully exploit the benefits of synthetic data, the effort to actually create these high-quality datasets should be kept at a minimum.

To mitigate additional effort of creating synthetic datasets on behalf of the researcher, software aimed at multiple imputation of missing data can be employed. Especially if researchers used this software at an earlier stage in the research process, or acquired familiarity with it during earlier projects, the additional burden of creating synthetic datasets is relatively small. The R-package `mice` [@mice] implements multiple imputation of missing data in a straightforward and user-friendly manner. However, the functionality of `mice` is not restricted to the imputation of missing data, but allows to impute any value in the data: even observed values. Consequently, `mice` can be utilized for the creation of multiply imputed synthetic datasets. 

After creating the multiply imputed synthetic datasets, the goal is to obtain valid statistical inferences in the spirit of @rubin_multiple_1987 and @neyman1934. In the missing data framework, this is done by performing statistical analyses on all imputed datasets, and pooling the results of the analyses according to Rubin's rules [@rubin_multiple_1987, pp.76]. In the synthetic data framework, the same procedure is followed, but with a slight twist: there are no values that remain constant over the synthetic datasets. The procedure of drawing valid inferences from multiple synthetic datasets is therefore slightly different. 

In this manuscript we detail a workflow for synthesizing data with `mice`. First, the `mice` algorithm for the creation of synthetic data will be shortly explained. The aim is to generate synthetic sets that reassure the privacy and confidentiality of the participants. Second, a straightforward workflow for imputation of synthetic data with `mice` will be demonstrated. Third, we demonstrate the validity of the procedure through statistical simulation.  

# Generating synthetic data with `mice`

**NEXT TWO PARAGRAPHS ARE PROBABLY NOT CORRECT YET; NOTE TO SELF: UPDATE THIS**

The `mice` package [@mice] in `R` [@Rproject] has been developed for multiple imputation of missing data. In this context, the aim is to replace missing values due to nonresponse by plausible values from the posterior predictive distribution of the variable containing the missings. Doing so, `mice` makes use of fully conditional specification [FCS; @vanbuuren_fully_2006], which breaks down the multivariate distribution of the data $\textbf{Y} = (\textbf{Y}_{obs}, \textbf{Y}_{mis})$ into $j = 1, 2, \dots, k$ univariate conditional densities, where $k$ denotes the number of columns in the data. Using FCS, a model is constructed for every incomplete variable and the missing values $Y_{j, mis}$ are then imputed with draws from the posterior predictive distribution of $P(Y_{j, mis} | \textbf{Y}_{obs}, \theta)$ on a variable-by-variable basis. Note that the predictor matrix $Y_{-j}$ may contain yet imputed values from an earlier imputation step, and thus will be updated after every iteration. This procedure is applied $m$ times, resulting in $m$ completed datasets $\textbf{D} = (\textbf{D}^{(1)}, \textbf{D}^{(2)}, \dots, \textbf{D}^{(m)})$, with $\textbf{D}^{(l)} = (\textbf{Y}_{obs}, Y^{(l)}_{mis})$.

In `mice`, the generation of multiply imputed datasets to solve for unobserved values is straightforward. The following pseudocode details the multiple imputation of the `mice::boys` dataset [@fredriks_boys_2000] into the object `imp` with `m = 10` imputated sets and `maxit = 7` iterations for the algorithm to converge, using the default imputations methods for each column data class.

```{r eval = FALSE}
library(mice)
imp <- mice(boys, 
            m = 10,
            maxit = 7)
```

It is straightforward to extended the imputation approach to generate synthetic values. Rather than imputing missing data, the observed values are then replaced by synthetic draws from the posterior predictive distribution. For simplicity, assume that the data is completely observed (i.e., $\textbf{Y} = \textbf{Y}_{obs}$). Following the notation of @reiter_raghunathan_multiple_2007, let for $n$ units denote $Z_i = 1$ if any of the values of unit $i = 1, 2, \dots, n$ are to be replaced by imputations, and $Z_i = 0$ otherwise, with $\textbf{Z} = (Z_1, Z_2, \dots, Z_n)$. Accordingly, the data consists of values that are to be replaced and values that are to be kept (i.e., $\textbf{Y} = (\textbf{Y}_{rep}, \textbf{Y}_{nrep})$. Now, instead of imputing $\textbf{Y}_{mis}$ with draws from the posterior predictive distribution of $P(Y_{j, mis} | \textbf{Y}_{obs}, \theta)$ as in the missing data case, $\textbf{Y}_{rep}$ is imputed from the posterior distribution of $P(Y^{(l)}_{j, rep} | \textbf{Y}^{(l)}_{-j}, \textbf{Z}, \theta)$, where $l$ is an indicator for the synthetic dataset ($l = 1, 2, \dots, m$). Note that synthetic values that are imputed at an earlier step can be used for imputing variable $j$. This process results in the synthetic data $\textbf{D} = (\textbf{D}^{(1)}, \textbf{D}^{(2)}, \dots, \textbf{D}^{(m)})$. 

For example, overimputing synthetic values for both the observed and missing cells in the `mice::boys` dataset into the object `syn`, given the same imputation parameters as before, can be realized by the following code execution.

```{r eval = FALSE}
syn <- mice(boys, 
            m = 10,
            maxit = 7, 
            where = matrix(TRUE, 
                           nrow = nrow(boys),
                           ncol = ncol(boys)))
```

where the argument `where` requires a matrix of the same dimensions as the data, (i.e., a $n \times k$ matrix) containing logicals $z_{ij}$ that indicate which cells are selected to have their values replaced by draws from the posterior predictive distribution. This approach allows to *overimpute* a subset of the observed data, or - as in the above example - the observed data as a whole, resulting in a dataset that partly or completely consists of synthetic data values. 

Choosing an adequate imputation model to impute the data is paramount, as a flawed imputation model may drastically impact the validity of inferences. Imputation models should be as flexible as possible to capture most of the patterns in the data, and to model possibly unanticipated data characteristics [@murray_multiple_2018; @rubin_18years_1996]. Parametric methods, albeit easy to implement in practice, may be too restrictive to capture generally complex patterns in the data, especially in the case of nonlinear relations and interactions between multiple variables. Classification and regression trees [CART; @breiman_cart_1984] allow to model more complex patterns in the data, and have therefore been suggested as an appropriate imputation method [@reiter_cart_2005; @burgette_reiter_cart_2010; @doove_buuren_recursive_2014]. Loosely speaking, CART sequentially splits the predictor space into non-overlapping regions in such a way that the within-region variance is as small as possible after every split. As such, CART does not impose any parametric distribution on the data, making it a widely applicable method that allows for a large variety of relationships within the data [@islr_2013]. Given these appealing characteristics and the call for the use of flexible methods when multiply imputing data, we will focus our illustrations and evaluations of `mice` to method `mice.impute.cart()`, realized by:

```{r eval = FALSE}
syn <- mice(boys, 
            m = 10,
            maxit = 7, 
            method = "cart",
            where = matrix(TRUE, 
                           nrow = nrow(boys),
                           ncol = ncol(boys)))
```


In a nutshell, the above code shows the simplicity of creating $m = 10$ synthetic datasets using `mice`. In practice, however, one should take some additional complicating factors into account. For example, one should account for deterministic relations in the data. Additionally, relations between variables may be described best using a different model than `CART`. Such factors are data dependent and should be considered by the imputer. In the next section, we will describe how the `boys` data can be adequately imputed. Additionally, we will show through simulations that this approach yields valid inferences.

# Simulations

We will show the suitability of using `mice` for synthesization using a simulation study, using the `boys` data. This dataset consists of the values of $`r nrow(mice::boys)`$ Dutch boys on the following $`r ncol(mice::boys)`$ variables: age (in years), height (in cm), weight (in kg), body mass index (BMI), head circumference (in cm), Genital Tanner stage (G1-G5), testicular volume (in ml) and region. Unfortunately, this dataset does not differ from the vast majority of collected datasets, in the sense that it suffers from missing data. For simplicity, the data is completed using the default `mice` imputation model for all predictors except BMI, which is imputed using a passive imputation model due to the deterministic relation with weight and height ($\text{BMI} = \frac{\text{weight}}{\text{height (in meters)}^2}$) to create a single, fully observed dataset:

```{r, include = FALSE}
library(mice)
library(magrittr)
library(tidyverse)
library(furrr)
library(knitr)
library(kableExtra)
```


```{r, results = FALSE, message = FALSE, warning = FALSE}
# create a single imputed, completely observed `boys` dataset
set.seed(123)

meth <- make.method(boys)
meth["bmi"] <- "~ I(wgt / (hgt / 100)^2)"
pred <- make.predictorMatrix(boys)
pred[c("hgt", "wgt"), "bmi"] <- 0

imp <- mice(boys, 
            m = 1,
            maxit = 10,
            method = meth,
            predictorMatrix = pred)

data <- complete(imp)
```

To induce an appropriate amount of sampling variance, 1000 bootstrap samples of the `boys` data will be synthesized with $m = 5$ imputations. The data will be synthesized using the `CART` imputation method for all predictors except BMI, which will be synthesized using passive imputation according to the model `~I(wgt / (hgt / 100)^2)`. Additional parameters that come with the use of `mice.impute.cart()` are the complexity parameter `cp` and the minimum number of observations in any terminal node `minbucket`, that both constrain the flexibility of the imputation model. The values of the parameters `cp` and `minbucket` ought to adhere to the call for imputation models that are as flexible as possible. Appropriate values for these parameters, as well as the input for the `predictorMatrix`, depend on the data at hand. In the current example, the complexity parameter is specified at `cp = 1e-08` rather than the default value `1e-04`, and the minimum number of observations in each terminal node is set at `minbucket` $= 3$ rather than the default value $5$. By allowing for more complexity in the imputation model, bias in the estimates from the synthetic dataset is reduced. Additionally, since the missingness pattern is monotone, the number of iterations can be set to `maxit = 1`.

To assess the performance of `mice` for synthesizing data, we compare the bootstrapped samples with the synthetic versions of these bootstrapped samples. Specifically, univariate descriptive statistics, the correlation matrix, and two linear regression models as well as one ordered logistic regression model will be considered. Subsequently, the bias in the parameters and the $95\%$ confidence interval coverage of the synthetic data will be examined. Similarly to multiple imputation of missing data, $95\%$ confidence intervals must be calculated using an adjusted variance estimator that accounts for the increase in variance due to imputation [@reiter_partially_inference_2003; @drechsler_synthetic_2011]. However, these variance estimators are not the same. Whereas the variance estimate after imputation of missing data needs to account for the fact that a certain amount of information in the data is missing, variance estimation from synthetic data does not suffer from this issue. The adjusted variance estimate that follows from using multiple synthetic datasets only suffers from the fact that a finite number of $m$ synthetic datasets are used to resemble the observed data. Hence, the according pooling rules for synthetic data as developed by @reiter_partially_inference_2003 are

\begin{aligned}

\bar{q}_m &= \sum_{l = 1}^m \frac{q^{(l)}}{m}, \\

b_m       &= \sum_{l = 1}^m \frac{(q^{(l)} - \bar{q}_m)^2}{(m - 1)}, \\

\bar{u}_m &= \sum_{l = 1} \frac{u^{(l)}}{m}.
\end{aligned}

```{r eval = FALSE}
syn <- mice(boys,
            m = 10,
            maxit = 7,
            method = "cart",
            where = matrix(TRUE,
                           nrow = nrow(boys),
                           ncol = ncol(boys)),
            cp = 1e-08,
            minbucket = 3)
```


# Simulation setup
- boys data met bootstrap om sampling variance er in te brengen
- wat evalueren we: bias | standard error | coverage
- welke pooling: T = Ubar + B/m
# psych::describe
# correlation matrix
# wgt ~ age + bmi | hgt ~ age + wgt | gen ~ age + 
# psych::describe - univariate ok
map --> psych::describe() --> Reduce("+", .) in de pipe

# correlation matrix - relations preserved
map --> cor --> logit --> pool --> inv.logit --> correct diagonal

# wgt ~ age + bmi | hgt ~ age + wgt | gen ~ age + bmi - multivariate ok
3 sets modellen

complete(imp, "all") 
--> lm1
--> lm2
--> polr

pooling cf. T = Ubar + B/m

# Conclusie
- We hebben laten zien dat het belangrijk is om meerdere synthetische versies te creeren. Dit is ook geen probleem, dat doen we immers al decennia lang met imputeren. 
- Waarom geen synthpop --> Ibrahim 2000
- werkt wel goed.
- Beperkingen
- Wat verder nog te doen: aanvullende functies voor complexere combinaties van synthetiseren en imputeren



```{r}
library(mice)
library(dplyr)
library(magrittr)
meth <- make.method(boys)
meth["bmi"] <- "~ I(wgt / (hgt / 100)^2)"
pred <- make.predictorMatrix(boys)
pred[c("hgt", "wgt"), "bmi"] <- 0
imp <-mice(boys, 
           meth = meth, 
           pred = pred, 
           print = FALSE)
with(imp, lm(wgt ~ age + bmi))%>% pool() %>% summary()
with(imp, lm(bmi ~ I((wgt / (hgt/100)^2))))%>% pool() %>% summary()
library(MASS)
with(imp, polr(gen ~ age + bmi, Hess = TRUE)) %>% pool() %>% summary()
```

# log reg prediction of rows



# Synthesizing data with `mice`

**!!PROBABLY NOT USED!!**

*To display the applicability of `mice` for generating synthetic data, we will display three distinct simulation set-ups that build up in complexity. The first case considers a multivariable regression model, in which the normally distributed outcome variable $y$ is regressed on three normally distributed predictor variables $\{x_1, x_2, x_3\}$, which are bivariately correlated with $\rho_{x_i, x_j} = 0.3$. The regression coefficients are specified in such a way that $\beta_2$ equals $2\beta_1$ and $\beta_3$ equals $3\beta_1$. Additionally, the proportion explained variance in $y$ equals $R^2=0.25$. Hence, the population regression weights equal $\beta_1 = 0.11$, $\beta_2 = 0.22$ and $\beta_3 = 0.33$.*

*The second simulation involves a logistic regression model, in which $y$ follows a Bernoulli distribution with probability*
$$
\frac{1}{1 + \text{exp}\{-X\beta\}},
$$
*with predictor matrix $X$, consisting of 4 predictors. Here, $x_3$ moderates the relationship between $x_2$ and $y$, while $x_4$ is transformed into a categorical variable with three levels.*




<!--

Now, we aim to generate the replacement values for the actually observed data $Y_{syn}$, instead of values that replace missing values $Y_{imp}$. 

Using FCS, for every variable containing missing values, a model is constructed to impute the missing values on a given variable conditional on all other variables in the data, and the posterior distribution of the parameters used in the model at hand, on a variable by variable basis, which yields $P(Y_{mis} | X, \theta)$. 


The `mice` package has been developed originally for multiple imputation of missing data, but can be easily extended to the synthetic data framework. It proceeds by drawing missing or synthetic values from the joint distribution of the observed data. This joint density is generally hard or impossible to derive, which can be overcome by employing fully conditional specification (FCS). Consider a $n \times k$ data matrix $Y$, with values to be replaced $Y^{old}$, the values to be kept $Y^{keep}$ and the values that are already replaced $Y^{new}$, under the assumption that there are no missing values in the observed dataset. Instead of drawing from the joint distribution, we draw from $k$ univariate conditional densities for each variable. That is, we draw from the distribution $P\big{(}Y_j^{old} | Y^{keep}, Y^{new}, Y^{old}_{-j}, \theta \big{)}$, where the drawn values are conditional upon the values that are to be kept, the synthetic values that are drawn at an earlier iteration and the values that still are to be replaced, with $j = 1, 2, \dots, k$ and $\theta$ indicating the parameter specifying the distribution of $Y_j^{old}$. Thus, the synthetic values are generated variable by variable. This procedure is executed $i = 1, 2, \dots, m$ times, resulting in $\textbf{D} = \{D^{(1)}, \dots, D^{(m)}\}$ synthetic datasets, with $D^{(i)} = (Y^{keep}, Y^{i, new})$.

To select the values that are to be replaced, one can utilize the `where` parameter in the `mice` function. This parameter allows one to set which values of the data are to be imputed. 

-->






KEY: Het moet niet mogelijk zijn om de echte waarden te reverse engineeren. De stochastische component van multiple imputation zorgt er voor dat dit onmogelijk is. Wat wel kan worden afgeleid is het ware data genererende model - en dat is nou precies de bedoeling. 



<!--

It is easy to draw



The goal then is to release multiply imputed datasets that bear no risk of releasing confidential or identifying information, while the multivariate relationships should be preserved as in the observed data.




The fact that the then released data is synthetic could potentially fulfil the needs for openly accessible data, without running into barriers with regard to privacy and confidentiality constraints.


, based on the multiple imputation framework [@rubin_multiple_1987]. 


Potentially, these approaches  However, there is no such thing as a free lunch: data collectors have to put effort in creating synthetic data, and using incorrect models to generate synthetic data might yield biased results for potential analysts of the data. It thus is crucial that the synthesizing process will be as easy as it gets, so that the step to release synthetic data is as small as possible.


An alternative has been offered by @rubin_statistical_disclosure_1993, who, building on the framework of multiple imputation, proposed to release multiple synthetic datasets to the public. Conform this approach, all units that are in the population but not in the sample are treated as missing data. These values are imputed by means of conventional multiple imputation approaches, and simple random samples are drawn from the population. Ultimately, these samples are released to the public. In practice, it is not required to impute the complete population, since random samples can be drawn from the sampling frame, so that only these sampled values have to be imputed. The datasets generated under this approach are labelled as *fully synthetic datasets*. However, note that, if one draws simple random samples from the sampling frame, the possibility exists that actual observations are included. 

A second procedure to creating synthetic datasets has been proposed by @little_statistical_1993, named *partially synthetic datasets*, which originated as a procedure that required synthesizing only those values that are at a high risk of being disclosive. These values can be complete variables that could be compared with publicly available datasets or registers, such as addresses, or it can be certain values that bear a high risk of being disclosive, say, income values above a certain threshold. Similarly to the fully synthetic data approach, multiple datasets containing synthetic values are released to the public, although in this instance, only the values at a high risk vary over the synthetic datasets. However, nothing keeps us from treating all values in the dataset as bearing a high disclosure risk, resulting in datasets that are completely existing of synthetic values. This approach basically implements the idea of fully synthetic data, based on the partially synthetic data procedure.

-- -- HIER MIST NOG EEN OVERGANG WAAR IK NOG NIET UIT BEN -- -- 


As of today, most papers in this field address methodological issues concerning both fully and partially synthetic datasets, both with regard to analysis methods suitable for creating synthetic data, and concerning inferential procedures. The current paper aims at reducing the gap between the theory about synthetic data, and generating synthetic data in practice. Due to the conceptual similarities between imputation for missing data and imputation for synthetic data, we propose that the R-package `mice` is capable of bridging the gap between theory and actual applications. Originally, `mice` is developed as an R-package to impute values subject to non-response. However, algorithmically, the procedure of imputing values subject to non-response does not need to differ from imputing synthetic data. Both approaches can be based on fully conditional specification (FCS) as implemented in `mice`. Conform this approach, values are drawn iteratively from the posterior predictive distribution of the variable of interest conditional on all other variables, for each variable separately. Thus, previously imputed values are taken into account in the imputation model for variables that are imputed later on. 

Since `mice` is currently only capable of imputing the data at hand, the approach should be labelled as *partially synthetic*. That is, although it is completely possible to create a synthetic dataset that consists of only synthetic values, `mice` is not (yet) capable of imputing observations that are not present in the data. Creating fully synthetic datasets by means of `mice` remains an area of future research.


-- -- NOT SURE YET: CONTRAST FULLY AND PARTIALLY SYNTHETIC DATA -- --



**Intro**

- Motivation/relevance

- Synthetic data (fully/partially shortly explained)

- How can this data be generated in practice (FCS - multivariate synthetic values)

- Mice adopts FCS for missing data

- It will be shown that mice is also capable of generating multivariate synthetic data

- However, multiple rules for correct variance estimates have been proposed, but the choice of one optimal variance estimate is not straightforward. Therefore, the performance of the currently available variance estimators (as outlined below) will be contrasted

- Goal: explain and show how mice can be used to create synthetic versions of the data at hand, and which variance estimator yields the best results.

**Synthetic data explained**

*Fully synthetic data*

- Explain the idea in more depth than in the intro
- Explain pooling rules by Raghunathan, Reiter & Rubin
- Adjusted variance estimate by Reiter (restricted positive)
- Simple variance estimator by `synthpop` authors.

*Partially synthetic data*

- Explain the idea in more depth than in the intro
- Explain pooling rules by Reiter.
- Adjusted estimate by `synthpop` authors.

**Mice**

 - Explain algorithm (FCS) in more depth
 - What is synthetic data
 - Why is it useful
 - Why should it be easy to generate
 - Parrallel to missing data
 - If similar to missing data then why not use mice
 - All fun and game, but statistical properties should be intact
 - In this paper we demonstrate a straigthforward workflow for generating synthetic datasets and we investigate how to obtain valid inferences from synthesized data. 
 
## Generating synthetic data with mice
- Explain FCS in short
- Demonstrate procedure with where matrix and explain rationale
- Demonstrate that it is a reasonable approach based figure that shows obs relations and synth relations side-by-side (e.g. wgt ~ age | gen)
- Observed values can be regarded as starting values for the imputation model. Since the observed data is used for creating the imputation models, no iterations are required. That is, the model is fixed, and not dependent on the imputed values.

## Drawing inferences from mice synthesized data
- What ways are there (with minimal - if any - escapes to partial/fully synthesized)
- If based on your earlier work, cite the online reference for your simulations and give URL. No need to go into detail about it here. 

### Simulation
- Simulation set-up
- Evaluation criteria explained: the bias cov ciw triangle
- Table with demonstrate bias cov ciw for all methods

## Discussion
- Summarize findings
- Stress that only holds for sampled data. Explain that for fully observed populations, one would need no sampling variance. 
- Revisit the simplicity and the readily available solution within the mice workflow (if adjusted pool is required, add to mice and direct to that function)


# FULLY/PARTIALLY synthetic data

## Fully synthetic data

DISCUSS APPROACH IN MORE DEPTH

Explain pooling rules (variance estimates + adjusted variance estimates + simple estimator proposed by synthpop authors)

## Partially synthetic data

DISCUSS APPROACH IN MORE DEPTH

Explain pooling rules (variance estimates)

# Mice

- Discuss the FCS algorithm in more detail.
- Discuss mice, predominantly the imputation algorithm in greater depth
- Procedure: observed values can be regarded as starting values, so that all variables can be used for the synthesis of all other variables.

# Simulations

## Methods

- Boys data - bootstrapped to obtain a population.
- explain simple synthetic data procedure
- explain imputation settings
- CART has been proposed by Reiter


## Results





## References

